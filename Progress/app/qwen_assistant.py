"""
ã€é€šä¹‰åƒé—® Qwenã€‘APIé›†æˆæ¨¡å—
ç”¨äºæ„å›¾ç†è§£å’Œä»»åŠ¡å¤„ç†
"""
import json
import re
import logging
import dashscope
from dashscope import Generation
from database import config
from Progress.utils.logger_utils import log_time, log_step, log_var, log_call
from Progress.utils.logger_config import setup_logger

# --- åˆå§‹åŒ–æ—¥å¿—å™¨ ---
logger = logging.getLogger("ai_assistant")

DASHSCOPE_API_KEY = config.api_key
DASHSCOPE_MODEL = config.model


class QWENAssistant:
    def __init__(self):
        if not DASHSCOPE_API_KEY:
            raise ValueError("ç¼ºå°‘ DASHSCOPE_API_KEYï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
        dashscope.api_key = DASHSCOPE_API_KEY

        self.model_name = DASHSCOPE_MODEL or 'qwen-max'
        logger.info(f"âœ… QWENAssistant åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {self.model_name}")

        self.conversation_history = []

        self.system_prompt = """
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è¯­éŸ³æ§åˆ¶åŠ©æ‰‹ï¼Œèƒ½å¤Ÿç†è§£ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„ä»»åŠ¡è®¡åˆ’ã€‚

ä½ çš„èŒè´£æ˜¯ï¼š
- å‡†ç¡®ç†è§£ç”¨æˆ·æ„å›¾ï¼›
- è‹¥æ¶‰åŠå¤šä¸ªåŠ¨ä½œï¼Œéœ€æ‹†è§£ä¸ºã€æ‰§è¡Œè®¡åˆ’ã€‘ï¼›
- è¾“å‡ºä¸€ä¸ªä¸¥æ ¼ç¬¦åˆè§„èŒƒçš„ JSON å¯¹è±¡ï¼Œä¾›ç³»ç»Ÿè§£ææ‰§è¡Œï¼›
- æ‰€æœ‰å›å¤å¿…é¡»ä½¿ç”¨ä¸­æ–‡ï¼ˆä»…é™äº response_to_user å­—æ®µï¼‰ï¼›

ğŸ¯ è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š
{
  "intent": "system_control",           // æ„å›¾ç±»å‹ï¼š"system_control"
  "task_type": "start_background_tasks",// ä»»åŠ¡ç±»å‹çš„ç®€è¦æè¿°ï¼ˆåŠ¨æ€ç”Ÿæˆï¼‰
  "execution_plan": [                   // æ‰§è¡Œæ­¥éª¤åˆ—è¡¨ï¼ˆæ¯ä¸ªæ­¥éª¤åŒ…å« operation, parameters, descriptionï¼‰
    {
      "operation": "å‡½æ•°å",             // å¿…é¡»æ˜¯å·²çŸ¥æ“ä½œä¹‹ä¸€
      "parameters": { ... },            // å‚æ•°å¯¹è±¡ï¼ˆæŒ‰éœ€æä¾›ï¼‰
      "description": "è¯¥æ­¥éª¤çš„ç›®çš„è¯´æ˜"
    }
  ],
  "response_to_user": "ä½ è¦å¯¹ç”¨æˆ·è¯´çš„è¯ï¼ˆç”¨ä¸­æ–‡ï¼‰",
  "requires_confirmation": false,       // æ˜¯å¦éœ€è¦ç”¨æˆ·ç¡®è®¤åå†æ‰§è¡Œ
  "mode": "parallel"                    // æ‰§è¡Œæ¨¡å¼ï¼š"parallel"ï¼ˆå¹¶è¡Œï¼‰æˆ– "serial"ï¼ˆä¸²è¡Œï¼‰
}

ğŸ“Œ å·²çŸ¥ operation åˆ—è¡¨ï¼ˆä¸å¯æ‹¼å†™é”™è¯¯ï¼‰ï¼š
- play_music(music_path: str)
- stop_music()
- pause_music()
- resume_music()
- open_application(app_name: str)
- create_file(file_name: str, content?: str)
- read_file(file_name: str)
- write_file(file_name: str, content: str)
- set_reminder(reminder_time: str, message: str)
- exit()

ğŸ“Œ è§„åˆ™è¯´æ˜ï¼š
1. åªæœ‰å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚æ‰§è¡Œç³»ç»Ÿçº§ä»»åŠ¡æ—¶ï¼Œæ‰è®¾ç½® intent="system_control"ï¼›
   å¦åˆ™è®¾ä¸º intent="chat"ï¼ˆä¾‹å¦‚é—²èŠã€é—®å¤©æ°”ã€è®²ç¬‘è¯ç­‰ï¼‰ã€‚

2. execution_plan ä¸­çš„æ¯ä¸€æ­¥éƒ½å¿…é¡»ä¸ç”¨æˆ·éœ€æ±‚ç›´æ¥ç›¸å…³ï¼›
   âŒ ç¦æ­¢æ·»åŠ æ— å…³æ“ä½œï¼ˆå¦‚éšä¾¿åŠ  speak_response æˆ– play_musicï¼‰ï¼

3. mode å†³å®šæ‰§è¡Œæ–¹å¼ï¼š
   - å¦‚æœå„æ­¥éª¤äº’ä¸ä¾èµ– â†’ "parallel"
   - å¦‚æœæœ‰å…ˆåä¾èµ–ï¼ˆå¦‚å…ˆæ‰“å¼€å†å†™å…¥ï¼‰â†’ "serial"

4. response_to_user æ˜¯ä½ å¯¹ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€åé¦ˆï¼Œå¿…é¡»ç®€æ´å‹å¥½ï¼Œä½¿ç”¨ä¸­æ–‡ã€‚

5. requires_confirmationï¼š
   - æ¶‰åŠåˆ é™¤ã€è¦†ç›–æ–‡ä»¶ã€é•¿æ—¶é—´è¿è¡Œä»»åŠ¡ â†’ true
   - æ™®é€šæ“ä½œï¼ˆæ‰“å¼€åº”ç”¨ã€æ’­æ”¾éŸ³ä¹ï¼‰â†’ false

âš ï¸ é‡è¦è­¦å‘Šï¼š
- ç»ä¸å…è®¸ç…§æ¬ç¤ºä¾‹ä¸­çš„å‚æ•°æˆ–è·¯å¾„ï¼å¿…é¡»æ ¹æ®ç”¨æˆ·è¾“å…¥æå–çœŸå®ä¿¡æ¯ã€‚
- ä¸å¾—è™šæ„ä¸å­˜åœ¨çš„ operation æˆ– parameter åç§°ã€‚
- ä¸å¾—çœç•¥ä»»ä½•å­—æ®µï¼Œæ‰€æœ‰ key éƒ½å¿…é¡»å­˜åœ¨ã€‚
- ä¸å¾—è¾“å‡ºé¢å¤–æ–‡æœ¬ï¼ˆå¦‚è§£é‡Šã€æ³¨é‡Šã€
```json
``` åŒ…è£¹ç¬¦ï¼‰ï¼Œåªè¾“å‡ºçº¯ JSON å¯¹è±¡ã€‚

âœ… æ­£ç¡®è¡Œä¸ºç¤ºä¾‹ï¼š

ç”¨æˆ·è¯´ï¼šâ€œå¸®æˆ‘å†™ä¸€ä»½è‡ªæˆ‘ä»‹ç»åˆ° D:/intro.txtï¼Œå¹¶æ‰“å¼€çœ‹çœ‹â€
â†’ åº”è¿”å›åŒ…å« write_file å’Œ read_file çš„ serial è®¡åˆ’ã€‚

ç”¨æˆ·è¯´ï¼šâ€œæ’­æ”¾ C:/Music/background.mp3 å¹¶å‘Šè¯‰æˆ‘å‡†å¤‡å¥½äº†â€
â†’ å¯ä»¥å¹¶è¡Œæ‰§è¡Œ play_music å’Œ speak_responseã€‚

ç”¨æˆ·è¯´ï¼šâ€œä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿâ€
â†’ intent="chat"ï¼Œresponse_to_user="æˆ‘å¾ˆå¥½ï¼Œè°¢è°¢ï¼"

ğŸš« é”™è¯¯è¡Œä¸ºï¼š
- æŠŠæ‰€æœ‰æŒ‡ä»¤éƒ½å˜æˆå’Œç¤ºä¾‹ä¸€æ ·çš„æ“ä½œç»„åˆï¼›
- åœ¨æ²¡æœ‰è¯·æ±‚çš„æƒ…å†µä¸‹è‡ªåŠ¨æ·»åŠ  speak_responseï¼›
- ä½¿ç”¨æœªå®šä¹‰çš„æ“ä½œå¦‚ run_scriptã€send_emailã€‚

ç°åœ¨ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„æœ€æ–°æŒ‡ä»¤ç”Ÿæˆå¯¹åº”çš„ JSON å“åº”ã€‚
"""

    @log_time
    @log_step("å¤„ç†è¯­éŸ³æŒ‡ä»¤")
    def process_voice_command(self, voice_text):
        log_var("åŸå§‹è¾“å…¥", voice_text)

        if not voice_text.strip():
            return self._create_fallback_response("æˆ‘æ²¡æœ‰å¬æ¸…æ¥šï¼Œè¯·é‡æ–°è¯´è¯ã€‚")

        self.conversation_history.append({"role": "user", "content": voice_text})

        try:
            messages = [{"role": "system", "content": self.system_prompt}]
            messages.extend(self.conversation_history[-10:])  # ä¿ç•™æœ€è¿‘ä¸Šä¸‹æ–‡

            response = Generation.call(
                model=self.model_name,
                messages=messages,
                temperature=0.5,
                top_p=0.8,
                max_tokens=1024
            )

            if response.status_code != 200:
                logger.error(f"Qwen API è°ƒç”¨å¤±è´¥: {response.status_code}, {response.message}")
                return self._create_fallback_response(f"æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {response.message}")

            ai_output = response.output['text'].strip()
            log_var("æ¨¡å‹è¾“å‡º", ai_output)

            self.conversation_history.append({"role": "assistant", "content": ai_output})

            # === å°è¯•è§£æ JSON ===
            parsed = self._extract_and_validate_json(ai_output)
            if parsed:
                return parsed
            else:
                # è‹¥æ— æ³•è§£æä¸ºæœ‰æ•ˆè®¡åˆ’ï¼Œåˆ™é™çº§ä¸ºæ™®é€šå¯¹è¯
                return self._create_fallback_response(ai_output)

        except Exception as e:
            logger.exception("å¤„ç†è¯­éŸ³æŒ‡ä»¤æ—¶å‘ç”Ÿå¼‚å¸¸")
            return self._create_fallback_response("æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚")

    def _extract_and_validate_json(self, text: str):
        """ä»æ–‡æœ¬ä¸­æå– JSON å¹¶éªŒè¯ç»“æ„"""
        try:
            # æ–¹æ³•1ï¼šç›´æ¥åŠ è½½
            data = json.loads(text)
            return self._validate_plan_structure(data)
        except json.JSONDecodeError:
            pass

        # æ–¹æ³•2ï¼šæ­£åˆ™åŒ¹é…ç¬¬ä¸€ä¸ªå¤§æ‹¬å·åŒ…è£¹çš„å†…å®¹
        match = re.search(r'\{[\s\S]*\}', text)
        if not match:
            return None

        try:
            data = json.loads(match.group())
            return self._validate_plan_structure(data)
        except:
            return None

    def _validate_plan_structure(self, data: dict):
        """éªŒè¯æ˜¯å¦ç¬¦åˆå¤šä»»åŠ¡è®¡åˆ’æ ¼å¼"""
        required_top_level = ["intent", "task_type", "execution_plan", "response_to_user", "requires_confirmation"]
        for field in required_top_level:
            if field not in data:
                logger.warning(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
                return None

        valid_operations = {
            "play_music", "stop_music", "pause_music", "resume_music",
            "open_application", "create_file", "read_file", "write_file",
            "set_reminder", "speak_response", "exit"
        }

        for step in data["execution_plan"]:
            op = step.get("operation")
            params = step.get("parameters", {})

            if not op or op not in valid_operations:
                logger.warning(f"æ— æ•ˆæ“ä½œ: {op}")
                return None

            if not isinstance(params, dict):
                logger.warning(f"parameters å¿…é¡»æ˜¯å¯¹è±¡: {params}")
                return None

        # è¡¥å…¨é»˜è®¤å€¼
        if "mode" not in data:
            data["mode"] = "parallel"

        return data

    def _create_fallback_response(self, message: str):
        """é™çº§å“åº”ï¼šç”¨äºéç»“æ„åŒ–è¾“å‡º"""
        return {
            "intent": "chat",
            "task_type": "reply",
            "response_to_user": message,
            "requires_confirmation": False,
            "execution_plan": [],
            "mode": "serial"
        }

    def _create_response(self, intent, action, parameters, response, needs_confirmation):
        resp = {"intent": intent, "action": action, "parameters": parameters, "response": response, "needs_confirmation": needs_confirmation}
        log_var("è¿”å›å“åº”", resp)
        return resp

    @log_time
    def generate_text(self, prompt, task_type="general"):
        log_var("ä»»åŠ¡ç±»å‹", task_type)
        log_var("æç¤ºè¯é•¿åº¦", len(prompt))

        try:
            system_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬ç”ŸæˆåŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·çš„è¦æ±‚ç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬å†…å®¹ã€‚

ä»»åŠ¡ç±»å‹ï¼š{task_type}
è¦æ±‚ï¼š{prompt}

è¯·ç”Ÿæˆç›¸åº”çš„æ–‡æœ¬å†…å®¹ï¼Œç¡®ä¿å†…å®¹å‡†ç¡®ã€æœ‰é€»è¾‘ã€è¯­è¨€æµç•…ã€‚
"""
            response = Generation.call(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.8,
                max_tokens=2000
            )

            if response.status_code == 200:
                result = response.output['text']
                log_var("ç”Ÿæˆç»“æœé•¿åº¦", len(result))
                return result
            else:
                error_msg = f"æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {response.message}"
                logger.error(error_msg)
                return error_msg

        except Exception as e:
            logger.exception("æ–‡æœ¬ç”Ÿæˆå‡ºé”™")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆæ–‡æœ¬æ—¶é‡åˆ°é”™è¯¯ï¼š{str(e)}"

    @log_time
    def summarize_text(self, text):
        log_var("å¾…æ€»ç»“æ–‡æœ¬é•¿åº¦", len(text))
        try:
            prompt = f"è¯·æ€»ç»“ä»¥ä¸‹æ–‡æœ¬çš„ä¸»è¦å†…å®¹ï¼š\n\n{text}"
            response = Generation.call(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=500
            )
            if response.status_code == 200:
                result = response.output['text']
                log_var("æ€»ç»“ç»“æœé•¿åº¦", len(result))
                return result
            else:
                error_msg = f"æ€»ç»“å¤±è´¥: {response.message}"
                logger.error(error_msg)
                return error_msg
        except Exception as e:
            logger.exception("æ–‡æœ¬æ€»ç»“å‡ºé”™")
            return f"æŠ±æ­‰ï¼Œæ€»ç»“æ–‡æœ¬æ—¶é‡åˆ°é”™è¯¯ï¼š{str(e)}"

    @log_time
    def translate_text(self, text, target_language="è‹±æ–‡"):
        log_var("ç›®æ ‡è¯­è¨€", target_language)
        log_var("åŸæ–‡é•¿åº¦", len(text))
        try:
            prompt = f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_language}ï¼š\n\n{text}"
            response = Generation.call(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=1000
            )
            if response.status_code == 200:
                result = response.output['text']
                log_var("ç¿»è¯‘ç»“æœé•¿åº¦", len(result))
                return result
            else:
                error_msg = f"ç¿»è¯‘å¤±è´¥: {response.message}"
                logger.error(error_msg)
                return error_msg
        except Exception as e:
            logger.exception("æ–‡æœ¬ç¿»è¯‘å‡ºé”™")
            return f"æŠ±æ­‰ï¼Œç¿»è¯‘æ–‡æœ¬æ—¶é‡åˆ°é”™è¯¯ï¼š{str(e)}"

assistant = QWENAssistant()
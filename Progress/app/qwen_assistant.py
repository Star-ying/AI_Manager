"""
ã€é€šä¹‰åƒé—® Qwenã€‘APIé›†æˆæ¨¡å—
ç”¨äºæ„å›¾ç†è§£å’Œä»»åŠ¡å¤„ç†ï¼ˆæ”¯æŒ expect_follow_up å­—æ®µï¼‰
"""

import json
import re
import logging
import dashscope
from dashscope import Generation

from database.config import config
from Progress.utils.logger_utils import log_time, log_step, log_var
from Progress.utils.logger_config import setup_logger

# --- åˆå§‹åŒ–æ—¥å¿—å™¨ ---
logger = logging.getLogger("ai_assistant")

DASHSCOPE_API_KEY = config.get("ai_model","api_key")
DASHSCOPE_MODEL = config.get("ai_model","model")


class QWENAssistant:
    def __init__(self):
        if not DASHSCOPE_API_KEY:
            raise ValueError("ç¼ºå°‘ DASHSCOPE_API_KEYï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
        dashscope.api_key = DASHSCOPE_API_KEY

        self.model_name = DASHSCOPE_MODEL or 'qwen-max'
        logger.info(f"âœ… QWENAssistant åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {self.model_name}")

        self.conversation_history = []

        self.system_prompt = """
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è¯­éŸ³æ§åˆ¶åŠ©æ‰‹ï¼Œèƒ½å¤Ÿç†è§£ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„ä»»åŠ¡è®¡åˆ’ã€‚

ä½ çš„èŒè´£æ˜¯ï¼š
- å‡†ç¡®ç†è§£ç”¨æˆ·æ„å›¾ï¼›
- è‹¥æ¶‰åŠå¤šä¸ªåŠ¨ä½œï¼Œéœ€æ‹†è§£ä¸ºã€æ‰§è¡Œè®¡åˆ’ã€‘ï¼›
- è¾“å‡ºä¸€ä¸ªä¸¥æ ¼ç¬¦åˆè§„èŒƒçš„ JSON å¯¹è±¡ï¼Œä¾›ç³»ç»Ÿè§£ææ‰§è¡Œï¼›
- æ‰€æœ‰å›å¤å¿…é¡»ä½¿ç”¨ä¸­æ–‡ï¼ˆä»…é™äº response_to_user å­—æ®µï¼‰ï¼›

ğŸ¯ è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š
{
  "intent": "system_control",           // æ„å›¾ç±»å‹ï¼š"system_control"
  "task_type": "start_background_tasks",// ä»»åŠ¡ç±»å‹çš„ç®€è¦æè¿°ï¼ˆåŠ¨æ€ç”Ÿæˆï¼‰
  "execution_plan": [                   // æ‰§è¡Œæ­¥éª¤åˆ—è¡¨
    {
      "operation": "å‡½æ•°å",
      "parameters": { ... },
      "description": "è¯¥æ­¥éª¤çš„ç›®çš„è¯´æ˜"
    }
  ],
  "response_to_user": "ä½ è¦å¯¹ç”¨æˆ·è¯´çš„è¯ï¼ˆç”¨ä¸­æ–‡ï¼‰",
  "requires_confirmation": false,
  "mode": "parallel",
  "expect_follow_up": true              // ğŸ”¥ æ–°å¢å­—æ®µï¼šæ˜¯å¦é¢„æœŸç”¨æˆ·ä¼šç»§ç»­æé—®ï¼Ÿ
}

ğŸ“Œ å·²çŸ¥ operation åˆ—è¡¨ï¼š
- play_music()
- stop_music()
- pause_music()
- resume_music()
- open_application(app_name: str)
- create_file(file_name: str, content: str)
- read_file(file_name: str)
- write_file(file_name: str, content: str)
- set_reminder(reminder_time: str, message: str)
- exit()

ğŸ“Œ è§„åˆ™è¯´æ˜ï¼š
1. intent="chat" ä»…ç”¨äºé—²èŠã€é—®å¤©æ°”ç­‰éæ“ä½œç±»è¯·æ±‚ã€‚
2. execution_plan å¿…é¡»ä¸ç”¨æˆ·éœ€æ±‚ç›´æ¥ç›¸å…³ï¼Œç¦æ­¢è™šæ„æˆ–æ·»åŠ æ— å…³æ“ä½œã€‚
3. mode: å¹¶è¡Œ(parallel)/ä¸²è¡Œ(serial)ï¼ŒæŒ‰ä¾èµ–å…³ç³»é€‰æ‹©ã€‚
4. requires_confirmation: åˆ é™¤ã€è¦†ç›–æ–‡ä»¶ç­‰é«˜é£é™©æ“ä½œè®¾ä¸º trueã€‚
5. expect_follow_up: âš ï¸ æ–°å¢å…³é”®å­—æ®µï¼

ğŸ”¥ å…³äº expect_follow_up çš„åˆ¤æ–­æ ‡å‡†ï¼š
- ç”¨æˆ·æ­£åœ¨è¿›è¡Œå¤šæ­¥æ“ä½œï¼ˆå¦‚â€œå¸®æˆ‘å†™ä¸€ç¯‡æ–‡ç« â€ â†’ å¯èƒ½æ¥ç€è¯´â€œä¿å­˜åˆ°æ¡Œé¢â€ï¼‰â†’ True
- ç”¨æˆ·æå‡ºå¼€æ”¾å¼é—®é¢˜ï¼ˆå¦‚â€œä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½â€ï¼‰â†’ True
- ç”¨æˆ·è¡¨è¾¾æœªå®Œæˆæ„Ÿï¼ˆå¦‚â€œè¿˜æœ‰å‘¢ï¼Ÿâ€ã€â€œç„¶åå‘¢ï¼Ÿâ€ã€â€œæ¥ä¸‹æ¥æ€ä¹ˆåŠâ€ï¼‰â†’ True
- æ˜ç¡®ç»“æŸè¯­å¥ï¼ˆå¦‚â€œå…³é—­ç¨‹åºâ€ã€â€œä¸ç”¨äº†â€ã€â€œè°¢è°¢â€ï¼‰â†’ False
- å•æ¡å‘½ä»¤å·²å®Œæˆé—­ç¯ï¼ˆå¦‚â€œæ‰“å¼€è®°äº‹æœ¬â€ï¼‰ä¸”æ— å»¶ä¼¸è¿¹è±¡ â†’ False

ğŸ’¡ ç¤ºä¾‹ï¼š
  ç”¨æˆ·ï¼šâ€œæˆ‘æƒ³å­¦ä¹  Pythonâ€
  â†’ expect_follow_up = True ï¼ˆç”¨æˆ·å¯èƒ½ç»§ç»­é—®æ€ä¹ˆå­¦ã€æ¨èä¹¦ç±ç­‰ï¼‰

  ç”¨æˆ·ï¼šâ€œæ’­æ”¾éŸ³ä¹â€
  â†’ expect_follow_up = True ï¼ˆå¯èƒ½ä¼šåˆ‡æ­Œã€æš‚åœï¼‰

  ç”¨æˆ·ï¼šâ€œé€€å‡ºâ€
  â†’ expect_follow_up = False

âš ï¸ é‡è¦è­¦å‘Šï¼š
- ç»ä¸å…è®¸çœç•¥ä»»ä½•å­—æ®µï¼›
- ä¸å¾—è¾“å‡ºé¢å¤–æ–‡æœ¬ï¼ˆå¦‚æ³¨é‡Šã€è§£é‡Šï¼‰ï¼›
- ä¸å…è®¸ä½¿ç”¨æœªçŸ¥ operationï¼›
- å¿…é¡»è¿”å›çº¯ JSONã€‚

ç°åœ¨ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„æœ€æ–°æŒ‡ä»¤ç”Ÿæˆå¯¹åº”çš„ JSON å“åº”ã€‚
"""

    @log_time
    @log_step("å¤„ç†è¯­éŸ³æŒ‡ä»¤")
    def process_voice_command(self, voice_text):
        log_var("åŸå§‹è¾“å…¥", voice_text)

        if not voice_text.strip():
            return self._create_fallback_response("æˆ‘æ²¡æœ‰å¬æ¸…æ¥šï¼Œè¯·é‡æ–°è¯´è¯ã€‚", expect_follow_up=False)

        self.conversation_history.append({"role": "user", "content": voice_text})

        try:
            messages = [{"role": "system", "content": self.system_prompt}]
            messages.extend(self.conversation_history[-10:])  # æœ€è¿‘10è½®ä¸Šä¸‹æ–‡

            response = Generation.call(
                model=self.model_name,
                messages=messages,
                temperature=0.5,
                top_p=0.8,
                max_tokens=1024
            )

            if response.status_code != 200:
                logger.error(f"Qwen API è°ƒç”¨å¤±è´¥: {response.status_code}, {response.message}")
                return self._create_fallback_response(f"æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {response.message}", expect_follow_up=False)

            ai_output = response.output['text'].strip()
            log_var("æ¨¡å‹è¾“å‡º", ai_output)

            self.conversation_history.append({"role": "assistant", "content": ai_output})

            # === è§£æå¹¶éªŒè¯ JSON ===
            parsed = self._extract_and_validate_json(ai_output)
            if parsed:
                return parsed
            else:
                # é™çº§å“åº”ï¼šå‡è®¾åªæ˜¯æ™®é€šèŠå¤©
                clean_text = re.sub(r'json[\s\S]*?|', '', ai_output).strip()
                return self._create_fallback_response(clean_text, expect_follow_up=True)

        except Exception as e:
            logger.exception("å¤„ç†è¯­éŸ³æŒ‡ä»¤æ—¶å‘ç”Ÿå¼‚å¸¸")
            return self._create_fallback_response("æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚", expect_follow_up=False)

    def _extract_and_validate_json(self, text: str):
        """ä»æ–‡æœ¬ä¸­æå– JSON å¹¶éªŒè¯ç»“æ„ï¼ˆå« expect_follow_upï¼‰"""
        try:
            data = json.loads(text)
            return self._validate_plan_structure(data)
        except json.JSONDecodeError:
            pass

        # å°è¯•æ­£åˆ™æå–ç¬¬ä¸€ä¸ªå¤§æ‹¬å·å†…å®¹
        match = re.search(r'\{[\s\S]*\}', text)
        if not match:
            return None
        try:
            data = json.loads(match.group())
            return self._validate_plan_structure(data)
        except:
            return None

    def _validate_plan_structure(self, data: dict):
        """éªŒè¯ç»“æ„å¹¶è¡¥å…¨å­—æ®µ"""
        required_top_level = ["intent", "task_type", "execution_plan", "response_to_user", "requires_confirmation"]
        for field in required_top_level:
            if field not in data:
                logger.warning(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
                return None

        valid_operations = {
            "play_music", "stop_music", "pause_music", "resume_music",
            "open_application", "create_file", "read_file", "write_file",
            "set_reminder", "exit"
        }

        for step in data["execution_plan"]:
            op = step.get("operation")
            params = step.get("parameters", {})

            if not op or op not in valid_operations:
                logger.warning(f"æ— æ•ˆæ“ä½œ: {op}")
                return None
            if not isinstance(params, dict):
                logger.warning(f"parameters å¿…é¡»æ˜¯å¯¹è±¡: {params}")
                return None

        # è¡¥å…¨é»˜è®¤å€¼
        if "mode" not in data:
            data["mode"] = "parallel"
        if "expect_follow_up" not in data:
            # å¯å‘å¼è¡¥å…¨
            ending_words = ['é€€å‡º', 'å…³é—­', 'åœæ­¢', 'æ‹œæ‹œ', 'å†è§', 'ä¸ç”¨äº†', 'è°¢è°¢']
            is_ending = any(word in data.get("response_to_user", "") for word in ending_words)
            data["expect_follow_up"] = not is_ending

        return data

    def _create_fallback_response(self, message: str, expect_follow_up: bool):
        """é™çº§å“åº”ï¼ŒåŒ…å« expect_follow_up å­—æ®µ"""
        return {
            "intent": "chat",
            "task_type": "reply",
            "response_to_user": message,
            "requires_confirmation": False,
            "execution_plan": [],
            "mode": "serial",
            "expect_follow_up": expect_follow_up  # ğŸ‘ˆ æ–°å¢
        }

    @log_time
    def generate_text(self, prompt, task_type="general"):
        log_var("ä»»åŠ¡ç±»å‹", task_type)
        log_var("æç¤ºè¯é•¿åº¦", len(prompt))
        try:
            system_prompt = f"ä½ æ˜¯ä¸“ä¸šæ–‡æœ¬ç”ŸæˆåŠ©æ‰‹ã€‚\nä»»åŠ¡ç±»å‹ï¼š{task_type}\nè¦æ±‚ï¼š{prompt}"
            response = Generation.call(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.8,
                max_tokens=2000
            )
            if response.status_code == 200:
                result = response.output['text']
                log_var("ç”Ÿæˆç»“æœé•¿åº¦", len(result))
                return result
            else:
                error_msg = f"æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {response.message}"
                logger.error(error_msg)
                return error_msg
        except Exception as e:
            logger.exception("æ–‡æœ¬ç”Ÿæˆå‡ºé”™")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆæ–‡æœ¬æ—¶é‡åˆ°é”™è¯¯ï¼š{str(e)}"

    @log_time
    def summarize_text(self, text):
        log_var("å¾…æ€»ç»“æ–‡æœ¬é•¿åº¦", len(text))
        try:
            prompt = f"è¯·æ€»ç»“ä»¥ä¸‹æ–‡æœ¬çš„ä¸»è¦å†…å®¹ï¼š\n\n{text}"
            response = Generation.call(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=500
            )
            if response.status_code == 200:
                result = response.output['text']
                log_var("æ€»ç»“ç»“æœé•¿åº¦", len(result))
                return result
            else:
                error_msg = f"æ€»ç»“å¤±è´¥: {response.message}"
                logger.error(error_msg)
                return error_msg
        except Exception as e:
            logger.exception("æ–‡æœ¬æ€»ç»“å‡ºé”™")
            return f"æŠ±æ­‰ï¼Œæ€»ç»“æ–‡æœ¬æ—¶é‡åˆ°é”™è¯¯ï¼š{str(e)}"

    @log_time
    def translate_text(self, text, target_language="è‹±æ–‡"):
        log_var("ç›®æ ‡è¯­è¨€", target_language)
        log_var("åŸæ–‡é•¿åº¦", len(text))
        try:
            prompt = f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_language}ï¼š\n\n{text}"
            response = Generation.call(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=1000
            )
            if response.status_code == 200:
                result = response.output['text']
                log_var("ç¿»è¯‘ç»“æœé•¿åº¦", len(result))
                return result
            else:
                error_msg = f"ç¿»è¯‘å¤±è´¥: {response.message}"
                logger.error(error_msg)
                return error_msg
        except Exception as e:
            logger.exception("æ–‡æœ¬ç¿»è¯‘å‡ºé”™")
            return f"æŠ±æ­‰ï¼Œç¿»è¯‘æ–‡æœ¬æ—¶é‡åˆ°é”™è¯¯ï¼š{str(e)}"

# å®ä¾‹åŒ–å…¨å±€åŠ©æ‰‹
assistant = QWENAssistant()
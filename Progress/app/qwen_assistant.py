"""
ã€é€šä¹‰åƒé—® Qwenã€‘APIé›†æˆæ¨¡å—
ç”¨äºæ„å›¾ç†è§£å’Œä»»åŠ¡å¤„ç†ï¼ˆæ”¯æŒ expect_follow_up å­—æ®µï¼‰
"""

import json
import re
import logging
import dashscope
from dashscope import Generation
from threading import Lock

from database.config import config
from Progress.utils.logger_utils import log_time, log_step, log_var
from Progress.utils.logger_config import setup_logger


try:
    from Progress.utils.ai_tools import FUNCTION_SCHEMA
except ImportError:
    FUNCTION_SCHEMA = []

logger = logging.getLogger("ai_assistant")

DASHSCOPE_API_KEY = config.get("ai_model", "api_key")
DASHSCOPE_MODEL = config.get("ai_model", "model")


class QWENAssistant:
    _instance = None
    _initialized = False
    _lock = Lock()  # çº¿ç¨‹é”ï¼Œä¿è¯çº¿ç¨‹å®‰å…¨

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                # åŒé‡æ£€æŸ¥é”å®šï¼ˆDouble-Checked Lockingï¼‰
                if cls._instance is None:
                    cls._instance = super(QWENAssistant, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        # é˜²æ­¢é‡å¤åˆå§‹åŒ–
        if QWENAssistant._initialized:
            return

        with QWENAssistant._lock:
            if QWENAssistant._initialized:
                return

            # === æ­£å¼åˆå§‹åŒ– ===
            if not DASHSCOPE_API_KEY:
                raise ValueError("ç¼ºå°‘ DASHSCOPE_API_KEYï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
            dashscope.api_key = DASHSCOPE_API_KEY
            self.model_name = DASHSCOPE_MODEL or 'qwen-max'
            logger.info(f"âœ… QWENAssistant å•ä¾‹åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {self.model_name}")

            self.conversation_history = []

            # åŠ¨æ€ç”Ÿæˆ operation åˆ—è¡¨æ–‡æœ¬
            operation_list_text = self._generate_operation_list()

            # æ„å»º system prompt
            self.system_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è¯­éŸ³æ§åˆ¶åŠ©æ‰‹ï¼Œèƒ½å¤Ÿç†è§£ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„ä»»åŠ¡è®¡åˆ’ã€‚

ğŸ¯ è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š
{{
  "intent": "system_control",
  "task_type": "start_background_tasks",
  "execution_plan": [
    {{
      "operation": "å‡½æ•°å",
      "parameters": {{ ... }},
      "description": "è¯¥æ­¥éª¤çš„ç›®çš„è¯´æ˜"
    }}
  ],
  "response_to_user": "ä½ è¦å¯¹ç”¨æˆ·è¯´çš„è¯ï¼ˆç”¨ä¸­æ–‡ï¼‰",
  "requires_confirmation": false,
  "mode": "parallel",
  "expect_follow_up": true
}}

ğŸ“Œ å·²çŸ¥ operation åˆ—è¡¨ï¼š
{operation_list_text}

ğŸ“Œ è§„åˆ™è¯´æ˜ï¼š
1. intent="chat" ä»…ç”¨äºé—²èŠã€‚
2. execution_plan å¿…é¡»ä¸éœ€æ±‚ç›¸å…³ã€‚
3. mode: parallel/serialã€‚
4. requires_confirmation: é«˜é£é™©æ“ä½œè®¾ä¸º trueã€‚
5. expect_follow_up: æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­æ˜¯å¦é¢„æœŸåç»­æé—®ã€‚

ğŸ”¥ å…³äº expect_follow_up çš„åˆ¤æ–­æ ‡å‡†ï¼š
- å¤šæ­¥æ“ä½œã€å¼€æ”¾å¼é—®é¢˜ â†’ True
- æ˜ç¡®ç»“æŸè¯­å¥ â†’ False

âš ï¸ é‡è¦è­¦å‘Šï¼š
- ä¸å¾—è¾“å‡ºé¢å¤–æ–‡æœ¬ï¼›
- ä¸å…è®¸ä½¿ç”¨æœªçŸ¥ operationï¼›
- å¿…é¡»è¿”å›çº¯ JSONã€‚

ç°åœ¨ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„æœ€æ–°æŒ‡ä»¤ç”Ÿæˆå¯¹åº”çš„ JSON å“åº”ã€‚
"""

            QWENAssistant._initialized = True

    def _generate_operation_list(self) -> str:
        """æ ¹æ® FUNCTION_SCHEMA ç”Ÿæˆæ“ä½œåˆ—è¡¨æè¿°"""
        lines = []
        for item in FUNCTION_SCHEMA:
            name = item["name"]
            desc = item["description"]
            params_desc = item.get("parameters", {})
            param_strs = [f"{k}: {v}" for k, v in params_desc.items()]
            params_display = "(" + ", ".join(param_strs) + ")" if param_strs else "()"
            lines.append(f"- {name}{params_display}ï¼š{desc}")
        return "\n".join(lines) if lines else "- æ— å¯ç”¨æ“ä½œ"

    @classmethod
    def get_instance(cls):
        """æä¾›å…¬å…±è®¿é—®ç‚¹ï¼Œç¡®ä¿åˆå§‹åŒ–å·²å®Œæˆ"""
        if cls._instance is None:
            raise RuntimeError(
                "QWENAssistant å°šæœªåˆå§‹åŒ–ï¼è¯·å…ˆç¡®ä¿æ‰€æœ‰ @ai_callable å‡½æ•°å·²æ³¨å†Œï¼Œå¹¶æ‰‹åŠ¨è§¦å‘ä¸€æ¬¡å®ä¾‹åŒ–ã€‚"
            )
        return cls._instance

    @log_time
    @log_step("å¤„ç†è¯­éŸ³æŒ‡ä»¤")
    def process_voice_command(self, voice_text):
        log_var("åŸå§‹è¾“å…¥", voice_text)

        if not voice_text.strip():
            return self._create_fallback_response("æˆ‘æ²¡æœ‰å¬æ¸…æ¥šï¼Œè¯·é‡æ–°è¯´è¯ã€‚", expect_follow_up=False)

        self.conversation_history.append({"role": "user", "content": voice_text})

        try:
            messages = [{"role": "system", "content": self.system_prompt}]
            messages.extend(self.conversation_history[-10:])

            response = Generation.call(
                model=self.model_name,
                messages=messages,
                temperature=0.5,
                top_p=0.8,
                max_tokens=1024
            )

            if response.status_code != 200:
                logger.error(f"Qwen API è°ƒç”¨å¤±è´¥: {response.status_code}, {response.message}")
                return self._create_fallback_response(f"æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {response.message}", expect_follow_up=False)

            ai_output = response.output['text'].strip()
            log_var("æ¨¡å‹è¾“å‡º", ai_output)

            self.conversation_history.append({"role": "assistant", "content": ai_output})

            parsed = self._extract_and_validate_json(ai_output)
            if parsed:
                return parsed
            else:
                clean_text = re.sub(r'json[\s\S]*?\n|', '', ai_output).strip()
                return self._create_fallback_response(clean_text, expect_follow_up=True)

        except Exception as e:
            logger.exception("å¤„ç†è¯­éŸ³æŒ‡ä»¤æ—¶å‘ç”Ÿå¼‚å¸¸")
            return self._create_fallback_response("æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚", expect_follow_up=False)

    def _extract_and_validate_json(self, text: str):
        try:
            data = json.loads(text)
            return self._validate_plan_structure(data)
        except json.JSONDecodeError:
            pass

        match = re.search(r'\{[\s\S]*\}', text)
        if not match:
            return None
        try:
            data = json.loads(match.group())
            return self._validate_plan_structure(data)
        except:
            return None

    def _validate_plan_structure(self, data: dict):
        required_top_level = ["intent", "task_type", "execution_plan", "response_to_user", "requires_confirmation"]
        for field in required_top_level:
            if field not in data:
                logger.warning(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
                return None

        valid_operations = {item["name"] for item in FUNCTION_SCHEMA} | {"exit"}

        for step in data["execution_plan"]:
            op = step.get("operation")
            params = step.get("parameters", {})

            if not op or op not in valid_operations:
                logger.warning(f"æ— æ•ˆæ“ä½œ: {op}")
                return None
            if not isinstance(params, dict):
                logger.warning(f"parameters å¿…é¡»æ˜¯å¯¹è±¡: {params}")
                return None

        if "mode" not in data:
            data["mode"] = "parallel"
        if "expect_follow_up" not in data:
            ending_words = ['é€€å‡º', 'å…³é—­', 'åœæ­¢', 'æ‹œæ‹œ', 'å†è§', 'ä¸ç”¨äº†', 'è°¢è°¢']
            is_ending = any(word in data.get("response_to_user", "") for word in ending_words)
            data["expect_follow_up"] = not is_ending

        return data

    def _create_fallback_response(self, message: str, expect_follow_up: bool):
        return {
            "intent": "chat",
            "task_type": "reply",
            "response_to_user": message,
            "requires_confirmation": False,
            "execution_plan": [],
            "mode": "serial",
            "expect_follow_up": expect_follow_up
        }

    @log_time
    def generate_text(self, prompt, task_type="general"):
        log_var("ä»»åŠ¡ç±»å‹", task_type)
        log_var("æç¤ºè¯é•¿åº¦", len(prompt))
        try:
            system_prompt = f"ä½ æ˜¯ä¸“ä¸šæ–‡æœ¬ç”ŸæˆåŠ©æ‰‹ã€‚\nä»»åŠ¡ç±»å‹ï¼š{task_type}\nè¦æ±‚ï¼š{prompt}"
            response = Generation.call(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.8,
                max_tokens=2000
            )
            if response.status_code == 200:
                result = response.output['text']
                log_var("ç”Ÿæˆç»“æœé•¿åº¦", len(result))
                return result
            else:
                error_msg = f"æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {response.message}"
                logger.error(error_msg)
                return error_msg
        except Exception as e:
            logger.exception("æ–‡æœ¬ç”Ÿæˆå‡ºé”™")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆæ–‡æœ¬æ—¶é‡åˆ°é”™è¯¯ï¼š{str(e)}"

    @log_time
    def summarize_text(self, text):
        log_var("å¾…æ€»ç»“æ–‡æœ¬é•¿åº¦", len(text))
        try:
            prompt = f"è¯·æ€»ç»“ä»¥ä¸‹æ–‡æœ¬çš„ä¸»è¦å†…å®¹ï¼š\n\n{text}"
            response = Generation.call(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=500
            )
            if response.status_code == 200:
                result = response.output['text']
                log_var("æ€»ç»“ç»“æœé•¿åº¦", len(result))
                return result
            else:
                error_msg = f"æ€»ç»“å¤±è´¥: {response.message}"
                logger.error(error_msg)
                return error_msg
        except Exception as e:
            logger.exception("æ–‡æœ¬æ€»ç»“å‡ºé”™")
            return f"æŠ±æ­‰ï¼Œæ€»ç»“æ–‡æœ¬æ—¶é‡åˆ°é”™è¯¯ï¼š{str(e)}"

    @log_time
    def translate_text(self, text, target_language="è‹±æ–‡"):
        log_var("ç›®æ ‡è¯­è¨€", target_language)
        log_var("åŸæ–‡é•¿åº¦", len(text))
        try:
            prompt = f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_language}ï¼š\n\n{text}"
            response = Generation.call(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=1000
            )
            if response.status_code == 200:
                result = response.output['text']
                log_var("ç¿»è¯‘ç»“æœé•¿åº¦", len(result))
                return result
            else:
                error_msg = f"ç¿»è¯‘å¤±è´¥: {response.message}"
                logger.error(error_msg)
                return error_msg
        except Exception as e:
            logger.exception("æ–‡æœ¬ç¿»è¯‘å‡ºé”™")
            return f"æŠ±æ­‰ï¼Œç¿»è¯‘æ–‡æœ¬æ—¶é‡åˆ°é”™è¯¯ï¼š{str(e)}"
